from torch import nn
from torch.nn import functional as F
from transformers import (
    T5ForConditionalGeneration, T5Tokenizer, T5Model
)
from transformers import T5Tokenizer, T5Config

model_name = 't5-base'


class T5ConstrainedGen(T5ForConditionalGeneration):
    def __init__(self, config, ):
        super(T5ConstrainedGen, self).__init__(config)
        self.config = T5Config
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.transformer = T5Model.T5ForConditionalGeneration(model_name)

    def adjust_logits_during_generation(self, logits, cur_len, max_length):
        if cur_len == 1 and self.config.force_bos_token_to_be_generated:
            self._force_token_ids_generation(logits, self.config.bos_token_id)
        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:
            self._force_token_ids_generation(logits, self.config.eos_token_id)
        return logits

    def _force_token_ids_generation(self, scores, token_id) -> None:
        """force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float("inf"))"""
        scores[:, [x for x in range(
            self.config.vocab_size) if x != token_id]] = -float("inf")
